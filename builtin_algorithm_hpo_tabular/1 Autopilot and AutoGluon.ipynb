{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbdb166-365d-44ea-9288-99a4baba1b56",
   "metadata": {},
   "source": [
    "### Direct Marketing in Banking - Propensity Modelling with Tabular Data\n",
    "\n",
    "# Part 1: SageMaker Autopilot and AutoGluon\n",
    "\n",
    "> *This notebook works well with the `Python 3 (Data Science 3.0)` kernel on SageMaker Studio*\n",
    "\n",
    "This workshop explores a tabular, [binary classification](https://en.wikipedia.org/wiki/Binary_classification) use-case with significant **class imbalance**: predicting which of a bank's customers are likely to respond to a targeted marketing campaign.\n",
    "\n",
    "In this first notebook, you'll tackle the challenge with two AutoML tools: [Amazon SageMaker Autopilot](https://aws.amazon.com/sagemaker/autopilot/) and SageMaker's [built-in AutoGluon-Tabular algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.html).\n",
    "\n",
    "## Contents\n",
    "\n",
    "> ‚ÑπÔ∏è **Tip:** You can use the Table of Contents panel in the left sidebar on JupyterLab / SageMaker Studio, to view and navigate sections\n",
    "\n",
    "1. **[Prepare our environment](#Prepare-our-environment)**\n",
    "1. **[Fetch the example dataset](#Fetch-the-example-dataset)**\n",
    "1. **[Starting fast with SageMaker Autopilot](#Starting-fast-with-SageMaker-Autopilot)**\n",
    "1. **[Diving deeper with AutoGluon-Tabular](#Diving-deeper-with-AutoGluon-Tabular)**\n",
    "1. **[Conclusions](#Conclusions)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15522d2-6c27-41ad-a2c9-7a46c628dcf4",
   "metadata": {},
   "source": [
    "## Prepare our environment\n",
    "\n",
    "To get started, we'll need to:\n",
    "\n",
    "- **Import** some useful libraries (as in any Python notebook)\n",
    "- **Configure** -\n",
    "    - The [Amazon S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#CoreConcepts) and folder where **data** should be stored (to keep our environment tidy)\n",
    "    - The [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) defining what **permissions** the jobs you create will have\n",
    "- **Connect** to AWS in general (with [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)) and SageMaker in particular (with the [sagemaker SDK](https://sagemaker.readthedocs.io/en/stable/)), to use the cloud services\n",
    "\n",
    "Run the cell below, to set these up.\n",
    "\n",
    "> ‚ÑπÔ∏è **Tip:** Just like in a regular [JupyterLab notebook](https://jupyterlab.readthedocs.io/en/stable/user/interface.html), you can run code cells by clicking in to target cell - and then pressing the play (‚ñ∂Ô∏è) button in the toolbar or `Shift+Enter` on the keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e1b6a-41fc-4fb1-8194-76c2d96579d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python Built-Ins:\n",
    "import json\n",
    "import time\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # General-purpose AWS SDK for Python\n",
    "import numpy as np  # For matrix operations and numerical processing\n",
    "import pandas as pd  # Tabular data utilities\n",
    "import sagemaker  # High-level SDK specifically for Amazon SageMaker\n",
    "from sagemaker.automl.automl import AutoML as AutoMLEstimator\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "# Local Helper Functions:\n",
    "import util\n",
    "\n",
    "# Setting up SageMaker parameters\n",
    "sgmk_session = sagemaker.Session()  # Connect to SageMaker APIs\n",
    "bucket_name = sgmk_session.default_bucket()  # Select an Amazon S3 bucket\n",
    "bucket_prefix = \"sm101/direct-marketing\"  # Location in the bucket to store our files\n",
    "sgmk_role = sagemaker.get_execution_role()  # IAM Execution Role to use for permissions\n",
    "\n",
    "print(f\"s3://{bucket_name}/{bucket_prefix}\")\n",
    "print(sgmk_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679792b-7352-4064-a449-035a68f2c6c4",
   "metadata": {},
   "source": [
    "## Fetch the example dataset\n",
    "\n",
    "This example uses the [UCI Bank Marketing Dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) as per: S. Moro, P. Cortez and P. Rita. *A Data-Driven Approach to Predict the Success of Bank Telemarketing.* Decision Support Systems, Elsevier, 62:22-31, June 2014.\n",
    "\n",
    "In the following cells we'll download the dataset locally, store it in Amazon S3, and **also** load a transformed copy into [Amazon SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/).\n",
    "\n",
    "> ‚ÑπÔ∏è **Tip:** You can train and deploy models in SageMaker **without using** SageMaker Feature Store, but we introduce it in this example to show you to a wider range of SageMaker features.\n",
    ">\n",
    "> Don't worry too much about the details of how the data loading is done here - but for the curious, you can check out the code behind these helper functions in  [util/data.py](util/data.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a4fcd-7c54-42d7-a6a9-09efe63fd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = util.data.fetch_sample_data()\n",
    "print(f\"Got: {raw_data_path}\\n\")\n",
    "\n",
    "print(\"Uploading raw dataset to Amazon S3:\")\n",
    "raw_data_s3uri = f\"s3://{bucket_name}/{bucket_prefix}/raw.csv\"\n",
    "!aws s3 cp {raw_data_path} {raw_data_s3uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c1a74-eb5f-4c9f-9bb4-bc147138f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "feature_group_name = \"sm101-direct-marketing\"\n",
    "print(\"Loading data to SageMaker Feature Store:\")\n",
    "\n",
    "# No need to re-run this if you've done it already - just set `feature_group_name` variable.\n",
    "util.data.load_sample_data(\n",
    "    raw_data_path,\n",
    "    fg_s3_uri=f\"s3://{bucket_name}/{bucket_prefix}/feature-store\",\n",
    "    feature_group_name=feature_group_name,\n",
    "    ignore_cols=[\n",
    "        \"duration\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e72d9-8b36-4dc3-a43f-ddf892598b95",
   "metadata": {},
   "source": [
    "> ‚è∞ **You don't have to wait** for this cell to finish running: As soon as you reach the `Ingesting data...` step, you're ready to continue on to the next section!\n",
    "\n",
    "‚ñ∂Ô∏è As soon as you reach the `Ingesting data...` stage, you'll be able to see your \"feature group\" in the SageMaker Feature Store catalog:\n",
    "\n",
    "- Select the triangular *SageMaker Resources* icon from the left sidebar in SageMaker Studio\n",
    "- Choose `Feature Store` from the drop-down menu and click `Open Feature store`\n",
    "\n",
    "Note you can explore the catalog either by \"feature group\" (table), or searching for individual features themselves. Descriptions and some tags have already been populated for you, based on the dataset description from UCI.\n",
    "\n",
    "![](img/feature-store-features.png \"Screenshot of SMStudio Feature Store UI showing feature catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0be11-d5a8-4fe4-9f8a-b1c9426d9b15",
   "metadata": {},
   "source": [
    "## Starting fast with SageMaker Autopilot\n",
    "\n",
    "> ‚ÑπÔ∏è **Tip: To skip the following manual steps** - Scroll down and you'll find code to create a similar setup through the API.\n",
    "\n",
    "Autopilot makes it easy to get started on tabular ML problems, even without extensive data preparation or writing any code. This is because:\n",
    "\n",
    "- Autopilot will automatically explore multiple data pre-processing options, algorithms, and hyperparameters for you - to identify a high-performing model\n",
    "- Even if you **do** want to perform some manual feature engineering first, Autopilot has direct integrations from [SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/) (SageMaker's low-code/no-code data preparation tool). You could explore this by creating a new Data Wrangler Flow from the launcher or File > New > Data Wrangler Flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cedf0c-ead8-41b0-9b88-154db3acabe6",
   "metadata": {},
   "source": [
    "‚ñ∂Ô∏è While your data finishes importing to SageMaker Feature Store, let's **start an Autopilot experiment using the raw CSV file**\n",
    "\n",
    "1. **Select** the triangular ‚ñΩ *SageMaker Resources* icon from the left sidebar in SageMaker Studio\n",
    "1. **Choose** `Experiments and trials` from the drop-down menu\n",
    "1. **Click** the `Create Autopilot Experiment` button\n",
    "    - If you don't see this button, you've probably drilled down into an experiment/trial: Look for the üè° home icon button just below the \"Experiments and trials\" drop-down, and click it to return to the top level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e4ae3-6a5c-4a62-9ef8-75c1d0337f8e",
   "metadata": {},
   "source": [
    "‚ñ∂Ô∏è In the first **Experiment and data details** step:\n",
    "\n",
    "- Choose an **Experiment name** - something like `sm101-autopilot-1` should be fine.\n",
    "- For **S3 location**, you can use the `Browse` button or just enter the URI from `raw_data_s3uri` earlier in this notebook.\n",
    "- Leave any other settings as default and click **Next**\n",
    "\n",
    "![](img/autopilot-01-select-data.png \"Screenshot of Autopilot workflow selecting the raw data CSV in S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b78a88-0396-410d-a055-bb0ee47a1df6",
   "metadata": {},
   "source": [
    "‚ñ∂Ô∏è In the next **Experiment and data details** step:\n",
    "\n",
    "- For the **Target** column, select `y`\n",
    "- **De-select** the features from `ignore_cols` earlier in this notebook, to **exclude** them from the model.\n",
    "    - ‚ö†Ô∏è As discussed on the [UCI dataset page](https://archive.ics.uci.edu/ml/datasets/bank+marketing), the `duration` field is particularly inappropriate to include in modelling as it leaks information about the target variable (calls with longer duration were more likely to be successful sales, and call duration cannot be known at the point of targeting which customers to approach. We also exclude the final 5 macro-economic variables as these may not be easily available at inference time.\n",
    "\n",
    "![](img/autopilot-02-select-features.png \"Screenshot of Autopilot workflow excluding ignored features and selecting y as target variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440e3cc-6c71-4dd8-baba-8e61d371f1a8",
   "metadata": {},
   "source": [
    "‚ñ∂Ô∏è In the next **Training method** step:\n",
    "\n",
    "- You can leave the default **Auto** selection method. Since this dataset is small, the Ensembling method will be automatically selected. If you like, you could create a second Experiment to compare both methods.\n",
    "\n",
    "![](img/autopilot-03-select-mode.png \"Select Auto mode in the UI, which will be equivalent to Ensembling for this small dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a7982-5327-4f29-a70d-b25b7fd398b9",
   "metadata": {},
   "source": [
    "‚ñ∂Ô∏è In the next **Deployment and advanced settings** step:\n",
    "\n",
    "- **IF you selected \"HPO\" training method in the previous screen**: Consider reducing the `Runtime > Max candidates` setting from 250 to ~50. The default setting would likely take several hours to train.\n",
    "- **Otherwise**, leave all settings as default\n",
    "\n",
    "‚ñ∂Ô∏è **Click next**, review your settings, and then **Create experiment**.\n",
    "\n",
    "![](img/autopilot-04-runtime.png \"Screenshot selecting smaller number of max candidates in runtime menu - only applicable for HPO jobs\")\n",
    "\n",
    "Alternatively, you may like to create Autopilot jobs through the [CreateAutoMLJob API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateAutoMLJob.html) or - as shown below - the high-level `AutoML` class in the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/api/training/automl.html):\n",
    "\n",
    "> ‚ö†Ô∏è You don't need to run the following cell if you already created an Autopilot job manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec436a-5082-4f25-aac6-13b318d0573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's no need to run this cell if you created an Autopilot job manually!\n",
    "autopilot = AutoMLEstimator(\n",
    "    role=sgmk_role,\n",
    "    target_attribute_name=\"y\",\n",
    "\n",
    "    # At the time of writing, the high-level Python SDK didn't support ensembling mode - so we'll\n",
    "    # use the HPO mode instead with limited max_candidates:\n",
    "    max_candidates=20,\n",
    "\n",
    "    # Optional params to keep the environment tidy:\n",
    "    base_job_name=\"sm101-autopilot\",\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}/autopilot\",\n",
    ")\n",
    "\n",
    "autopilot.fit(raw_data_s3uri, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385544cd-1bf1-44b1-a1dd-fa25ab982354",
   "metadata": {},
   "source": [
    "## Diving deeper with AutoGluon-Tabular\n",
    "\n",
    "Another useful tool to build highly-accurate models quickly is the open-source [AutoGluon framework](https://auto.gluon.ai/stable/index.html) and the SageMaker built-in [AutoGluon-Tabular algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.html).\n",
    "\n",
    "As outlined in the [2020 paper by Erickson, Mueller et al](https://arxiv.org/abs/2003.06505), AutoGluon-Tabular is an advanced model stacking ensembling framework that beat 99% of participating data scientists in benchmark Kaggle contests with just 4hrs of model training.\n",
    "\n",
    "In fact at the time of writing, SageMaker Autopilot makes use of AutoGluon under the hood when running in ensembling mode: But you can also use AutoGluon directly as shown here for more customized experiments.\n",
    "\n",
    "\n",
    "### Understand the algorithm requirements\n",
    "\n",
    "The first step to using any SageMaker built-in algorithm is understanding its overall characteristics and the interface it offers. Here we'll refer to:\n",
    "\n",
    "- The [algorithm docs](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.html) to understand the **detail** of the **data formats** and **(hyper)-parameters** it supports - as well as sample notebooks\n",
    "- The [Common Parameters doc](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html) to compare the **high-level configurations** and capabilities between algorithms.\n",
    "\n",
    "For some built-in algorithms, a container `image_uri` is enough to configure. However as described in the [how to use](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.html#autogluon-tabular-modes) page, [SageMaker JumpStart-based](https://sagemaker.readthedocs.io/en/stable/overview.html#use-built-in-algorithms-with-pre-trained-models-in-sagemaker-python-sdk) algorithms like AutoGluon-Tabular also need a `script_uri` and `model_uri`.\n",
    "\n",
    "These resources are all pre-built, and we can look them up by the `retrieve()` functions in the SageMaker Python SDK as shown below.\n",
    "\n",
    "The default [hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular-hyperparameters.html) for the algorithm can also be loaded through the SDK, and below we make some minor customizations ready for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98063d16-2adf-4ce4-b704-c1db2eb67df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, script_uris, model_uris\n",
    "from sagemaker.hyperparameters import retrieve_default as retrieve_default_hyperparams\n",
    "\n",
    "ag_model_id, ag_model_version, train_scope = (\n",
    "    \"autogluon-classification-ensemble\",\n",
    "    \"*\",\n",
    "    \"training\",\n",
    ")\n",
    "training_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=ag_model_id,\n",
    "    model_version=ag_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "print(train_image_uri)\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=ag_model_id, model_version=ag_model_version, script_scope=train_scope\n",
    ")\n",
    "print(train_source_uri)\n",
    "# Retrieve the pre-trained model tarball to further fine-tune. In tabular case, however, the pre-trained model tarball is dummy and fine-tune means training from scratch.\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=ag_model_id, model_version=ag_model_version, model_scope=train_scope\n",
    ")\n",
    "print(train_model_uri)\n",
    "\n",
    "# Retrieve the default hyper-parameters for training the model\n",
    "hyperparameters = retrieve_default_hyperparams(\n",
    "    model_id=ag_model_id, model_version=ag_model_version\n",
    ")\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"auto_stack\"] = \"True\"\n",
    "hyperparameters[\"save_space\"] = \"True\"\n",
    "print(\"\\n\", hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc7b804-1adc-4fca-b534-032edce80295",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract batch data from the SageMaker Feature Store\n",
    "\n",
    "Next, we'll extract a snapshot of data from the (offline/batch) SageMaker Feature Store via serverless SQL query with [Amazon Athena](https://aws.amazon.com/athena/), to prepare for model training.\n",
    "\n",
    "Feature Store **tracks the history** of records, allowing you to reproduce point-in-time snapshots even when features change over time.\n",
    "\n",
    "- **Example queries** for time-travel and other views are available through the SageMaker Studio Feature Store UI: From your Feature Group, switch to the \"Sample queries\" tab.\n",
    "- The additional `event_time`, `write_time`, `api_invocation_time`, `is_deleted` and `row_number` fields returned in the below query are metadata for this history tracking - so won't be used in the actual model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c144afc-a7e9-420e-93d9-3cdb22fac8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group = FeatureGroup(feature_group_name, sagemaker_session=sgmk_session)\n",
    "query = feature_group.athena_query()\n",
    "table_name = query.table_name\n",
    "\n",
    "data_extract_s3uri = f\"s3://{bucket_name}/{bucket_prefix}/data-extract\"\n",
    "!aws s3 rm --quiet --recursive {data_extract_s3uri}  # Clear any previous extractions\n",
    "print(f\"Querying feature store to extract snapshot at:\\n{data_extract_s3uri}\")\n",
    "query.run(\n",
    "    f\"\"\"\n",
    "    SELECT *\n",
    "    FROM\n",
    "        (SELECT *,\n",
    "        row_number()\n",
    "        OVER\n",
    "            (PARTITION BY \"customer_id\"\n",
    "            ORDER BY \"event_time\" DESC, Api_Invocation_Time DESC, write_time DESC)\n",
    "        AS row_number\n",
    "        FROM \"sagemaker_featurestore\".\"{table_name}\"\n",
    "        WHERE \"event_time\" <= {time.time()})\n",
    "    WHERE row_number = 1 AND NOT is_deleted;\n",
    "    \"\"\",\n",
    "    output_location=data_extract_s3uri,\n",
    ")\n",
    "query.wait()\n",
    "\n",
    "full_df = query.as_dataframe()\n",
    "print(f\"Got {len(full_df)} records\")\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17151b2f-40d5-4390-8717-25ead7d4ce80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split and prepare datasets\n",
    "\n",
    "From the [Input and Output Interface section](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.html#InputOutput-AutoGluon-Tabular) of the algorithm doc, we know that AutoGluon-Tabular expects **CSV data in a particular structure**: `train/` and `validation/` folders each containing a single `data.csv`, with **no headers**, and the **target column first** in the files.\n",
    "\n",
    "Here we'll split the raw data snapshot into randomly shuffled training, validation, and test sets - and upload to S3 in the required format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03b27f-96b2-48ca-b4e4-80b3545e7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_data = full_df.drop(\n",
    "    columns=[\n",
    "        # Drop Feature Store metadata fields that aren't relevant to the model:\n",
    "        \"customer_id\", \"event_time\", \"write_time\", \"api_invocation_time\", \"is_deleted\", \"row_number\"\n",
    "    ],\n",
    "    errors=\"ignore\",  # Your DF may not have 'row_number' if you did a simple 'select * from' query\n",
    ")\n",
    "df_model_data\n",
    "\n",
    "# Shuffle and split dataset\n",
    "train_data, validation_data, test_data = np.split(\n",
    "    df_model_data.sample(frac=1, random_state=1729),\n",
    "    [int(0.7 * len(df_model_data)), int(0.9 * len(df_model_data))],\n",
    ")\n",
    "\n",
    "# Create CSV files for Train / Validation / Test\n",
    "train_data.to_csv(\"data/train.csv\", index=False, header=False)\n",
    "validation_data.to_csv(\"data/validation.csv\", index=False, header=False)\n",
    "test_data.to_csv(\"data/test.csv\", index=False, header=False)\n",
    "\n",
    "df_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d587ef5-4c4d-4fa5-a877-e93b85bdb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_s3uri = f\"s3://{bucket_name}/{bucket_prefix}/model-data-ag\"\n",
    "\n",
    "# Upload data to Amazon S3:\n",
    "train_data_s3uri = model_data_s3uri + \"/train/data.csv\"\n",
    "train_data.to_csv(train_data_s3uri, index=False, header=False)\n",
    "validation_data_s3uri = model_data_s3uri + \"/validation/data.csv\"\n",
    "validation_data.to_csv(validation_data_s3uri, index=False, header=False)\n",
    "test_data_s3uri = model_data_s3uri + \"/test/data.csv\"\n",
    "test_data.to_csv(test_data_s3uri, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557239c8-2c83-44b0-be71-ac84d24b1825",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "With the data prepared in a compatible format, and the parameters collected, we're ready to run a training job through the SageMaker SDK [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) class, which provides a high-level wrapper over the underlying [SageMaker CreateTrainingJob API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html).\n",
    "\n",
    "The training job runs on **separate, containerized infrastructure** from this notebook:\n",
    "\n",
    "- **You specify** the number and type of instances, and the IAM permissions with which the job runs (which could be separate from the notebook execution role)\n",
    "- The job is **independent** from the notebook: The input parameters, logs, metrics, and output artifacts are still available through the APIs even if the notebook disconnects/restarts part way through. (See [Estimator.attach(...)](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Estimator.attach) classmethod for re-attaching to previous/ongoing jobs).\n",
    "- A range of **other infrastructure parameters** are available like:\n",
    "    - [SageMaker managed spot](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html), to optimize infrastructure costs\n",
    "    - [Warm pool keep-alive](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html), to speed up start of sequential jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4f5ed-b223-4045-92fa-f63282ce5dc3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ag_estimator = sagemaker.estimator.Estimator(\n",
    "    base_job_name=\"autogluon\",\n",
    "    role=sgmk_role,  # IAM role for job permissions\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}/train-output\",  # Optional artifact output loc\n",
    "\n",
    "    image_uri=train_image_uri,  # AutoGluon-Tabular algorithm container\n",
    "    source_dir=train_source_uri,  # AutoGluon-Tabular script bundle (pre-built)\n",
    "    model_uri=train_model_uri,  # AutoGluon-Tabular pre-trained artifacts\n",
    "    entry_point=\"transfer_learning.py\",  # Training script in the source_dir\n",
    "\n",
    "    hyperparameters=hyperparameters,\n",
    "\n",
    "    instance_type=training_instance_type,  # Type of compute instance\n",
    "    instance_count=1,\n",
    "    max_run=25 * 60,  # Limit job to 25 minutes\n",
    ")\n",
    "\n",
    "# Launch a SageMaker Training job by passing the S3 path of the datasets:\n",
    "ag_estimator.fit({\"training\": model_data_s3uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963e739-5376-4c7a-b367-e810dc434e3e",
   "metadata": {},
   "source": [
    "As well as the logs streamed to the notebook, you can follow the status of the job in:\n",
    "- The [Training > Training jobs page of the AWS Console for SageMaker](https://console.aws.amazon.com/sagemaker/home?#/jobs)\n",
    "    - Including links to Amazon CloudWatch console to drill in to job logs and metric graphs\n",
    "- The Resources > Experiments and trials pane in SageMaker Studio\n",
    "    - Jobs started without an explicit Experiment configuration will appear under the \"Unassigned trial components\" folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8378171-0415-4313-a08c-ad9cd649e63b",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "When the training job is completed successfully, your model is ready to use for inference either in batch or real-time.\n",
    "\n",
    "For this particular algorithm, the **container URI and script are different** for inference than training, so we need to look up the inference artifacts similarly to training above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2901f70-b7ad-424b-867f-d1bf67b7469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_instance_type = \"ml.m5.large\"\n",
    "\n",
    "inference_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=ag_model_id,\n",
    "    model_version=ag_model_version,\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "print(inference_image_uri)\n",
    "\n",
    "inference_src_uri = script_uris.retrieve(\n",
    "    model_id=ag_model_id, model_version=ag_model_version, script_scope=\"inference\"\n",
    ")\n",
    "print(inference_src_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244743e-196d-401d-a044-8e74960efda8",
   "metadata": {},
   "source": [
    "Although you could deploy in **one line** with `ag_predictor = ag_estimator.deploy(...)`, this encapsulates [multiple steps](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) of generating a Model, Endpoint Configuration, and Endpoint.\n",
    "\n",
    "We'll explicitly separate out the model step here, which will be helpful for storing model metadata later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671eeb1-07bd-4354-9389-48a6e2589b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_model = ag_estimator.create_model(\n",
    "    image_uri=inference_image_uri,\n",
    "    source_dir=inference_src_uri,\n",
    "    entry_point=\"inference.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6822e40-3c55-4177-a38c-6317cf26abfa",
   "metadata": {},
   "source": [
    "Whether from a Model or direct from an Estimator, setting up a real-time endpoint for the trained model is just one `.deploy(...)` function call as shown below.\n",
    "\n",
    "> ‚è∞ This deployment might take **up to 5-10 minutes**, and by default the code will wait for the deployment to complete.\n",
    "\n",
    "If you like, you can instead:\n",
    "\n",
    "- Un-comment the `wait=False` parameter (or if you already ran the cell, press the ‚èπ \"stop\" button in the toolbar above)\n",
    "- Use the [Endpoints page of the SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/endpoints) to check the status of the deployment\n",
    "- Skip over the *Evaluation* section below (which won't run until the deployment is complete), and start the Hyperparameter Optimization job - which will take a while to run too, so can be started in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199539a0-a518-4074-8edf-8197a792aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ag_predictor = ag_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    \n",
    "    # wait=False,  # Remember, predictor.predict() won't work until deployment finishes!\n",
    "\n",
    "    # We will also turn on data capture here, in case you want to experiment with monitoring later:\n",
    "    data_capture_config=sagemaker.model_monitor.DataCaptureConfig(\n",
    "        enable_capture=True,\n",
    "        sampling_percentage=100,\n",
    "        destination_s3_uri=f\"s3://{bucket_name}/{bucket_prefix}/data-capture\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bde499-eab4-4270-b9f5-ad512509f32d",
   "metadata": {},
   "source": [
    "### Use the endpoint\n",
    "\n",
    "The [Predictor](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) class in the SageMaker SDK provides a high-level Python wrapper for creating and invoking inference endpoints which is useful in notebooks... Although consumer applications can also use the low-level [SageMaker Runtime API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_Operations_Amazon_SageMaker_Runtime.html) (with [Boto3 in Python](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html), or other AWS SDKs in other languages) to avoid this extra library dependency.\n",
    "\n",
    "SageMaker is a general-purpose ML platform supporting a wide range of use-cases beyond tabular data, so we need to explicitly configure what type of content we're sending in endpoint requests (here CSV) and specifying for the response (here JSON) - via the SDK [serializer](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html) and [deserializer classes](https://sagemaker.readthedocs.io/en/stable/api/inference/deserializers.html). You can also define your own classes to fully customize the Python I/O interface of the `predictor.predict()` method and the on-the-wire data format transmitted to the HTTPS endpoint. \n",
    "\n",
    "Again, when using a pre-built algorithm, refer to the [algorithm docs](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.html#InputOutput-AutoGluon-Tabular) to see what input and output formats are supported at inference time.\n",
    "\n",
    "When using `application/json` with the `verbose` flag, AutoGluon-Tabular can return **both** the predicted class labels and the class probabilities, which we'll use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88553a5a-2f41-4863-9f55-b9ae35489109",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "ag_predictor.deserializer = sagemaker.deserializers.JSONDeserializer(\n",
    "    accept=\"application/json;verbose\"\n",
    ")\n",
    "\n",
    "X_test_numpy = test_data.drop([\"y\"], axis=1).values\n",
    "\n",
    "model_response = ag_predictor.predict(X_test_numpy)\n",
    "\n",
    "print(\"Response keys:\", model_response.keys())\n",
    "\n",
    "# probabilities is (N, 2) with probs for both classes, so convert to 1D probability of cls '1':\n",
    "probabilities = np.array(model_response[\"probabilities\"], dtype=float)[:, 1].squeeze()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca883914-ff89-4d7a-aa1d-ba575f36148b",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è **Note:** The above single `predict()` call makes a single `InvokeEndpoint` request with the **entire test dataset in one batch**. That's fine for a small dataset like this one, but practical use-cases will need to balance between throughput efficiency (large batches reduce communication overhead), endpoint memory requirements, and payload size limits (6MB for real-time endpoints, at the time of writing).\n",
    "\n",
    "Our model has calculated probability scores (in the interval [0,1]) of a potential customer enrolling for a term deposit, and also assigned labels based on the assumed best threshold:\n",
    "\n",
    "- 0: The person **will not** enroll\n",
    "- 1: The person **will** enroll (making them a good candidate for direct marketing)\n",
    "\n",
    "If we like, we could stitch these predictions back on to the original dataframe to explore performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5a640-4aa7-45e0-b4fa-7ddbadc38999",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.concat(\n",
    "    [\n",
    "        pd.Series(probabilities, name=\"y_prob\", index=test_data.index),\n",
    "        pd.Series(model_response[\"predicted_label\"], name=\"y_pred\", index=test_data.index),\n",
    "        test_data,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "test_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673f064-a8ea-4219-b24e-482a9fa24c86",
   "metadata": {},
   "source": [
    "From this joined data we can calculate standard quality metrics to measure the performance of the classifier.\n",
    "\n",
    "The utility function below generates a graphical report here in the notebook, but also saves a JSON file in [SageMaker Model Quality Metrics compatible-format](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html): Similar to if we'd run a [SageMaker Model Quality Monitoring job](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf15b1-4253-4b6f-984d-9336692cc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = util.reporting.generate_binary_classification_report(\n",
    "    y_real=test_data[\"y\"].values,\n",
    "    y_predict_proba=probabilities,\n",
    "    # Since this model already outputs both labels and probabilities, we can use both:\n",
    "    y_predict_label=test_results[\"y_pred\"].values,\n",
    "    # No need for an arbitrary decision threshold:\n",
    "    # decision_threshold=0.5,\n",
    "    class_names_list=[\"Did not enroll\", \"Enrolled\"],\n",
    "    title=\"AutoGluon model\",\n",
    ")\n",
    "\n",
    "# Store the model quality report locally and on Amazon S3:\n",
    "with open(\"data/report-autogluon.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "model_quality_s3uri = f\"s3://{bucket_name}/{bucket_prefix}/{ag_model.name}/model-quality.json\"\n",
    "!aws s3 cp data/report-autogluon.json {model_quality_s3uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cddcf67-4a63-4300-acc7-350c59371088",
   "metadata": {},
   "source": [
    "### Register and share the model\n",
    "\n",
    "The trained model is already available in the SageMaker APIs to deploy and re-use (you should see it, for example, in the [Models page of the SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/models)).\n",
    "\n",
    "However, we can improve discoverability and governance by cataloging it in the [SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html). Here extra metadata can be associated, including I/O formats and the model quality report generated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2199e4e7-7f86-4076-b974-3c0bacc6c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"application/json\", \"application/json;verbose\"],\n",
    "    model_package_group_name=\"sm101-dm\",\n",
    "    description=\"Initial AutoGluon-Tabular model\",\n",
    "    model_metrics=sagemaker.model_metrics.ModelMetrics(\n",
    "        model_statistics=sagemaker.model_metrics.MetricsSource(\n",
    "            content_type=\"application/json\",\n",
    "            s3_uri=model_quality_s3uri,\n",
    "        ),\n",
    "    ),\n",
    "    domain=\"MACHINE_LEARNING\",\n",
    "    task=\"CLASSIFICATION\",\n",
    "    sample_payload_url=test_data_s3uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbc0c2-a74f-4680-8c31-37a5e354827d",
   "metadata": {},
   "source": [
    "You can explore and manage your versioned registry model packages in SageMaker Studio from the *SageMaker Resources > Model registry* tab: Including **reviewing and approving** new versions to trigger automated deployments.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "In this notebook, we saw how [**SageMaker Autopilot**](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html) and the [AutoGluon-Tabular **built-in algorithm**](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.html) can accelerate new ML projects: Helping you build a highly accurate model fast, without lots of manual feature engineering.\n",
    "\n",
    "- In the case of Autopilot, you don't even need to write code to get started: Just upload your data to Amazon S3 and work through the `Create experiment` UI flow.\n",
    "- When using built-in algorithms, **refer to the [algorithm's doc pages](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html)** for important usage info like data formats, and whether multi-instance training parallelism is supported.\n",
    "\n",
    "We also saw brief intros to how [SageMaker Feature Store](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html) can help catalog shared feature data, and how [SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html) helps with tracking and managing trained models. For more information on these MLOps features, you can refer to the documentation and the official [SageMaker notebook examples repository](https://github.com/aws/amazon-sagemaker-examples).\n",
    "\n",
    "‚ñ∂Ô∏è In [Notebook 2 XGBoost and HPO.ipynb](2%20XGBoost%20and%20HPO.ipynb), you'll see another SageMaker built-in algorithm in action and learn how SageMaker can automatically tune hyperparameters. Follow along to learn more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ee733-482f-4b2e-bda7-12d4f43ee603",
   "metadata": {},
   "source": [
    "## Releasing cloud resources\n",
    "\n",
    "While training job clusters are shut down automatically when the job stops, inference endpoints stay provisioned until explicitly deleted.\n",
    "\n",
    "To avoid unnecessary charges, un-comment and run the following code to clean up your AutoGluon model endpoint when finished experimenting. You can also check the [Inference > Endpoints page of the SageMaker console](https://console.aws.amazon.com/sagemaker/home?#/endpoints) for any other running endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64a6ee-4bef-49fb-bd0c-fc1861c709d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
