{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Keras MNIST Classifier - Local Example\n",
    "\n",
    "This notebook trains and exports a Keras CNN-based classifier for the MNIST DIGITS dataset, performing all storage and computation here on the notebook instance where you run it.\n",
    "\n",
    "To give a better idea for how you might apply the example to real-world problems, we follow through the data preparation and model build process manually - rather than relying on Keras built-in functions and data classes.\n",
    "\n",
    "Can you figure out how to re-create this workflow using SageMaker more effectively?\n",
    "\n",
    "See the accompanying **Instructions** notebook for more guidance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Using TensorFlow version {tf.__version__}\")\n",
    "print(f\"Keras version {tf.keras.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "Let's use the Keras built-in to load the MNIST data, but explore exactly what format that gives us:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"x_train.shape {x_train_raw.shape}; dtype {x_train_raw.dtype}\")\n",
    "print(f\"y_train.shape {y_train_raw.shape}; dtype {y_train_raw.dtype}\")\n",
    "print(f\"x_test.shape {x_test_raw.shape}; dtype {x_test_raw.dtype}\")\n",
    "print(f\"y_test.shape {y_test_raw.shape}; dtype {y_test_raw.dtype}\")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plt.hist(x_train_raw.flatten())\n",
    "ax.set_title(\"Histogram of Training Image Data\")\n",
    "ax.set_ylabel(\"Frequency in Training Set\")\n",
    "ax.set_xlabel(\"Pixel Value\")\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.hist(y_train_raw)\n",
    "ax.set_title(\"Histogram of Training Set Labels\")\n",
    "ax.set_ylabel(\"Frequency in Training Set\")\n",
    "ax.set_xlabel(\"Y Label Value\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data is pretty evenly distributed between labels 0-9, and our images are encoded by fixed-size 28x28 uint8 matrices from 0 to 255. Here we'll just plot a few examples to get a feel for them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some example images:\")\n",
    "fig = plt.figure(figsize=(14, 2))\n",
    "for i in range(5):\n",
    "    fig = plt.subplot(1, 5, i + 1)\n",
    "    ax = plt.imshow(x_train_raw[i], cmap=\"gray\")\n",
    "    fig.set_title(f\"Number {y_train_raw[i]}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "Rather than just using this data in the nice pre-prepared format, let's assume MNIST is just a stand-in for a real image classification dataset... We'll save the images to disk in per-label folders, like a real dataset might be:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/train\n",
    "!rm -rf data/test\n",
    "\n",
    "# This can take a while due to the number of images\n",
    "def save_to_disk(x, y, base_folder):\n",
    "    \"\"\"Save an image classification dataset to disk as JPEGs in label-named folders\"\"\"\n",
    "    for ix in range(len(y)):\n",
    "        label_str = \"digit-%d\" % y[ix]\n",
    "        os.makedirs(os.path.join(base_folder, label_str), exist_ok=True)\n",
    "        tf.keras.preprocessing.image.save_img(\n",
    "            os.path.join(base_folder, label_str, \"%s-%06d.jpg\" % (label_str, ix)),\n",
    "            # This function expects a channels dimension, which we'll add in first:\n",
    "            np.expand_dims(x[ix], 0),\n",
    "            data_format=\"channels_first\"\n",
    "        )\n",
    "\n",
    "print(\"Saving training data...\")\n",
    "os.makedirs(\"data/train\", exist_ok=True)\n",
    "save_to_disk(x_train_raw, y_train_raw, \"data/train\")\n",
    "print(\"Saving test data...\")\n",
    "os.makedirs(\"data/test\", exist_ok=True)\n",
    "save_to_disk(x_test_raw, y_test_raw, \"data/test\")\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "There wouldn't be much point saving the data in a typical format if we didn't load it in to our training process that way too! Let's do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May as well clear out the old memory, in case you're running a small instance:\n",
    "x_train_raw = None\n",
    "y_train_raw = None\n",
    "x_test_raw = None\n",
    "y_test_raw = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "labels = sorted(os.listdir(\"data/train\"))\n",
    "n_labels = len(labels)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "print(\"Loading label \", end=\"\")\n",
    "for ix_label in range(n_labels):\n",
    "    label_str = labels[ix_label]\n",
    "    print(f\"{label_str}...\", end=\"\")\n",
    "    trainfiles = filter(\n",
    "        lambda s: s.endswith(\".jpg\"),\n",
    "        os.listdir(os.path.join(\"data/train\", label_str))\n",
    "    )\n",
    "    for filename in trainfiles:\n",
    "        # Can't just use tf.keras.preprocessing.image.load_img(), because it doesn't close its file\n",
    "        # handles! So get \"Too many open files\" error... Grr\n",
    "        with open(os.path.join(\"data/train\", label_str, filename), \"rb\") as imgfile:\n",
    "            x_train.append(\n",
    "                # Squeeze (drop) that extra channel dimension, to be consistent with prev format:\n",
    "                np.squeeze(tf.keras.preprocessing.image.img_to_array(\n",
    "                    Image.open(imgfile)\n",
    "                ))\n",
    "            )\n",
    "            y_train.append(ix_label)\n",
    "    # Repeat for test data:\n",
    "    testfiles = filter(\n",
    "        lambda s: s.endswith(\".jpg\"),\n",
    "        os.listdir(os.path.join(\"data/test\", label_str))\n",
    "    )\n",
    "    for filename in testfiles:\n",
    "        with open(os.path.join(\"data/test\", label_str, filename), \"rb\") as imgfile:\n",
    "            x_test.append(\n",
    "                np.squeeze(tf.keras.preprocessing.image.img_to_array(\n",
    "                    Image.open(imgfile)\n",
    "                ))\n",
    "            )\n",
    "            y_test.append(ix_label)\n",
    "print()\n",
    "\n",
    "print(\"Shuffling trainset...\")\n",
    "train_shuffled = [(x_train[ix], y_train[ix]) for ix in range(len(y_train))]\n",
    "np.random.shuffle(train_shuffled)\n",
    "\n",
    "x_train = np.array([datum[0] for datum in train_shuffled])\n",
    "y_train = np.array([datum[1] for datum in train_shuffled])\n",
    "train_shuffled = None\n",
    "\n",
    "print(\"Shuffling testset...\")\n",
    "test_shuffled = [(x_test[ix], y_test[ix]) for ix in range(len(y_test))]\n",
    "np.random.shuffle(test_shuffled)\n",
    "\n",
    "x_test = np.array([datum[0] for datum in test_shuffled])\n",
    "y_test = np.array([datum[1] for datum in test_shuffled])\n",
    "test_shuffled = None\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we go ahead**, let's just quickly validate that the data is the same distribution as the original... Just shuffled in order:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = tf.keras.datasets.mnist.load_data()\n",
    "print(f\"x_train.shape {x_train_raw.shape}; dtype {x_train_raw.dtype}\")\n",
    "print(f\"y_train.shape {y_train_raw.shape}; dtype {y_train_raw.dtype}\")\n",
    "print(f\"x_test.shape {x_test_raw.shape}; dtype {x_test_raw.dtype}\")\n",
    "print(f\"y_test.shape {y_test_raw.shape}; dtype {y_test_raw.dtype}\")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plt.hist(x_train_raw.flatten())\n",
    "ax.set_title(\"Histogram of Training Image Data\")\n",
    "ax.set_ylabel(\"Frequency in Training Set\")\n",
    "ax.set_xlabel(\"Pixel Value\")\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.hist(y_train_raw)\n",
    "ax.set_title(\"Histogram of Training Set Labels\")\n",
    "ax.set_ylabel(\"Frequency in Training Set\")\n",
    "ax.set_xlabel(\"Y Label Value\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Some example images:\")\n",
    "fig = plt.figure(figsize=(14, 2))\n",
    "for i in range(5):\n",
    "    fig = plt.subplot(1, 5, i + 1)\n",
    "    ax = plt.imshow(x_train[i], cmap=\"gray\")\n",
    "    fig.set_title(f\"Number {y_train[i]}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the distributions haven't shifted, and the advertised labels still visually match the images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process the Data for our CNN\n",
    "\n",
    "We've recovered the dataset from our JPEG files back to the MNIST original format, and verified nothing's majorly broken...\n",
    "\n",
    "Next, we'll tweak this format for our neural network: Normalizing pixel values to improve the numerical conditioning, and one-hot encoding our labels to suit a softmax classifier output.\n",
    "\n",
    "Note in particular that our model expects both a batch dimension (for processing multiple samples in parallel) and a channel dimension (e.g. as if this were a 3-channel RGB image, except single-channel for grayscale) - as well as the X and Y axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're actually feeding the images in to nets this time, we should actually pay attention\n",
    "# to which way around Keras wants the channel dimension:\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    x_train = np.expand_dims(x_train, 1)\n",
    "    x_test = np.expand_dims(x_train, 1)\n",
    "else:\n",
    "    x_train = np.expand_dims(x_train, len(x_train.shape))\n",
    "    x_test = np.expand_dims(x_test, len(x_test.shape))\n",
    "\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"input_shape:\", input_shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, n_labels)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, n_labels)\n",
    "\n",
    "print(\"n_labels:\", n_labels)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model\n",
    "\n",
    "At its core, the model is a 2D convolutional network with a softmax output layer that'll yield a confidence score for every possible label (e.g. 10 options for digit = 0 to 9).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_labels, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer=tf.keras.optimizers.Adadelta(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model\n",
    "\n",
    "Keras makes fitting and evaluating the model straightforward enough: We don't have any fancy hooks, and are happy with the default logging:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    verbose=1, # Hint: You might prefer =2 for running in SageMaker!\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Model\n",
    "\n",
    "Keras has a built-in `model.save()` command, which in TensorFlow v2 can directly produce TensorFlow Serving-compatible outputs!\n",
    "\n",
    "...However, this notebook runs TensorFlow v1. To save you the frustration of figuring it out (there's a nice blog post on the subject [here](https://aws.amazon.com/blogs/machine-learning/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/)), we'll give you a hint by saving the model here in TensorFlow Serving-ready format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Running this code will clear your TF session!\n",
    "\n",
    "from tensorflow.python.saved_model.builder import SavedModelBuilder\n",
    "from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "# The export folder needs to be empty, or non-existent\n",
    "!rm -rf data/model/export\n",
    "\n",
    "# Note the export/modelID/version portions of the path are required structure for TFServing:\n",
    "export_path = f\"data/model/export/my-model/1\"\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "# Freeze learning and take a copy of the model:\n",
    "K.set_learning_phase(0)\n",
    "config = model.get_config()\n",
    "weights = model.get_weights()\n",
    "model_copy = Sequential.from_config(config)\n",
    "model_copy.set_weights(weights)\n",
    "\n",
    "builder = SavedModelBuilder(export_path)\n",
    "signature = predict_signature_def(\n",
    "    inputs={ \"inputs\": model_copy.input },\n",
    "    outputs={ \"score\": model_copy.output }\n",
    ")\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    # Save the meta graph and variables\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess=sess,\n",
    "        tags=[tag_constants.SERVING],\n",
    "        signature_def_map={ \"serving_default\": signature }\n",
    "    )\n",
    "    builder.save()\n",
    "    print(\"Model exported\")\n",
    "\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
